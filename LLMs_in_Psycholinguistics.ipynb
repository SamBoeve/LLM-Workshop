{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLLPcwcDaTPTpcV6toIHNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamBoeve/LLM-Workshop/blob/main/LLMs_in_Psycholinguistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **From Tool to Theory: LLMs in Psycholinguistics**\n",
        "\n"
      ],
      "metadata": {
        "id": "bwNVuxQuxORi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before we start\n",
        "\n",
        "Here I'll illustrate the three main use cases of LLMs in psycholinguistics.\n",
        "\n",
        "For each, I'll download a separate model and tokenizer from the Hugging Face website. Model names and additional info can be found on the website:\n",
        "https://huggingface.co/\n"
      ],
      "metadata": {
        "id": "MJGlc6xx01WQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFdAVzqHmfrk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import AutoModel, AutoConfig, AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Usecase 1\n",
        "\n",
        "## **LLMs as participants ~ Prompting**\n"
      ],
      "metadata": {
        "id": "Mn8yKKuR4M1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask a large language model how familiar it is with certain words.\n",
        "\n",
        "I used this paper as inspiration for the prompt:\n",
        "\n",
        "Conde, J., Mart√≠nez, G., Grandury, M., Arriaga-Prieto, C. & Haro, J., Schroeder, S., Hintz, F., Reviriego, P. & Brysbaert, M. (2025). Updating the German psycholinguistic word toolbox with AI-generated estimates of familiarity, concreteness, valence, arousal, and age of acquisition. 10.13140/RG.2.2.18709.64489."
      ],
      "metadata": {
        "id": "uxTXKbK5qU1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a model finetuned for question answering\n",
        "model_checkpoint = \"google/flan-t5-base\"\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "O1RgQw6Awyz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of words to rate\n",
        "words = [\"labyrinth\", \"mysterious\", \"president\", \"education\", \"eloquent\", \"kitchen\"]\n",
        "\n",
        "# Loop over the words\n",
        "for word in words:\n",
        "    prompt = f\"\"\"\n",
        "    Complete the following task as a native speaker of English.\n",
        "    Familiarity is a measure of how familiar something is.\n",
        "    Please indicate how familiar you think each English word is to a native English speaker on a scale from 1 (VERY UNFAMILIAR) to 7 (VERY FAMILIAR),\n",
        "    with the midpoint representing moderate familiarity.\n",
        "    Typically people are very unfamiliar with words like acumen or ostentatious, while they are more familiar with words like spoon or university.\n",
        "    Only answer a number from 1 to 7. Please limit your answer to numbers.\n",
        "    The English word is: {word}. What familiarity rating would you give this word?\n",
        "    \"\"\"\n",
        "    print(prompt)\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Run the input through the model\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=5,\n",
        "        temperature=0.1,\n",
        "        # do_sample=True,       # enable sampling\n",
        "        # num_beams=5,          # explore multiple continuations\n",
        "        # temperature=0.7,      # allow moderate diversity\n",
        "        # top_p=0.9,\n",
        "        # num_return_sequences=3\n",
        "    )\n",
        "\n",
        "    # Decode the model's output\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(result)\n",
        "\n",
        "    # for i, output in enumerate(outputs):\n",
        "    #   print(f\"Sample {i+1}: {tokenizer.decode(output, skip_special_tokens=True).strip()}\")\n"
      ],
      "metadata": {
        "id": "bPqK_ek8zgLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "\n",
        "These results don't make a lot of sense. There are a few potential reasons:\n",
        "\n",
        "1. We used a small, open-source model (200 million parameters). Larger models might produce better results. For reference, GPT-4o has approximately (200 billion parameters) but this model is not open-source. Models like this you can access through an API call.\n",
        "\n",
        "2. The model is trained to answer questions but estimating word familiarity is not part of the standard training set. Finetuning the model on existing human familiarity judgements could improve the results.\n",
        "\n",
        "3. Prompt engineering: changes to the prompt could improve (or worsen) the responses.\n",
        "\n",
        "4. Other sampling strategies could have an impact on the outputs."
      ],
      "metadata": {
        "id": "KUPiH6b3Bip7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Usecase 2\n",
        "\n",
        "## **Representations**"
      ],
      "metadata": {
        "id": "GoesH21R4jRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this we load a masked language model\n",
        "model_checkpoint = \"google-bert/bert-base-uncased\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "model = BertModel.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "ajjhEM16475o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the model represents the word *bank* in two different sentences **prior** to contextualizing the input.\n",
        "\n",
        "Before working its magic, a language model uses a standard word embedding vector to represent a word (e.g., word2vec).\n",
        "\n",
        "Remember, for now these word vectors are not contextualized."
      ],
      "metadata": {
        "id": "hEo6hYFDpx3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Two sentences with differnt use of the word 'bank'\n",
        "sent1 = \"He deposited money in the bank.\"\n",
        "sent2 = \"He admired the flowers on the river bank.\"\n",
        "\n",
        "# First, we tokenize both sentences\n",
        "word_id1 = tokenizer.encode(sent1, add_special_tokens=False)\n",
        "word_id2 = tokenizer.encode(sent2, add_special_tokens=False)\n",
        "print(word_id1)\n",
        "print(word_id2)\n",
        "\n",
        "# Next, convert the token IDs to their associated word embeddings\n",
        "# Note that these are non-contextualized word embeddings\n",
        "static_embedding1 = model.embeddings.word_embeddings.weight[word_id1]\n",
        "static_embedding2 = model.embeddings.word_embeddings.weight[word_id2]\n",
        "\n",
        "# We get a tensor with dimension [sequence length, embedding dimension]\n",
        "print(static_embedding1.shape)\n",
        "print(static_embedding2.shape)\n",
        "\n",
        "# Compute the cosine similarity\n",
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "output = cos(static_embedding1[5, :], static_embedding2[7, :]).item()\n",
        "print(f\"Cosine similarity: {output:.2f}\")\n",
        "\n",
        "# Show the first five dimensions of the word embedding of 'bank' in both sentences\n",
        "print(static_embedding1[5, :5])\n",
        "print(static_embedding2[7, :5])"
      ],
      "metadata": {
        "id": "2FkRZhO0G7WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how the vector representations of the word *bank* changes **after** we run the sentence through the BERT model."
      ],
      "metadata": {
        "id": "QZ4WSUGnsf30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'bank'\n",
        "\n",
        "sentences = [\n",
        "    \"He deposited money in the bank.\",\n",
        "    \"He admired the flowers on the river bank.\"\n",
        "]\n",
        "\n",
        "contextual_embeddings_list = []\n",
        "\n",
        "for sent in sentences:\n",
        "  # Tokenize the input\n",
        "  inputs = tokenizer(sent, return_tensors=\"pt\")\n",
        "\n",
        "  # Run the input through the model\n",
        "  outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "  # Collect the model's hidden states (i.e., the contextualized embeddings)\n",
        "  last_hidden_states = outputs.hidden_states[-1]\n",
        "\n",
        "  # Find token positions corresponding to the target word\n",
        "  tokenized = tokenizer.tokenize(sent)\n",
        "  positions = [i for i, t in enumerate(tokenized) if t == target]\n",
        "  print(positions)\n",
        "\n",
        "  # Collect embeddings of the target word\n",
        "  contextual_embedding = last_hidden_states[0, positions, :]\n",
        "\n",
        "  # Store the embeddings\n",
        "  contextual_embeddings_list.append(contextual_embedding)\n",
        "\n",
        "# Compute cosine similarity between the two contextualized embeddings\n",
        "sim_between_contexts = cos(contextual_embeddings_list[0], contextual_embeddings_list[1])\n",
        "print(f\"Cosine similarity between 'bank' in the two contexts: {sim_between_contexts.item():.2f}\")"
      ],
      "metadata": {
        "id": "9DN8X8nZIYtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usecase 3\n",
        "\n",
        "## **Word Probabilities**"
      ],
      "metadata": {
        "id": "X2iknL6s4vn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load an autoregressive model\n",
        "model_checkpoint = \"openai-community/gpt2\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "sWG2mfV3O9St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll just extract the top ten predictions of the model based on a given sentence frame."
      ],
      "metadata": {
        "id": "dmnxqn1wBISS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' This is a'\n",
        "\n",
        "# Tokenize the input\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "print(input_ids)\n",
        "# tensor([[770, 318, 257]])\n",
        "\n",
        "# Run the input through the model\n",
        "outputs = model(input_ids)\n",
        "print(outputs.logits.shape)\n",
        "# outputs.logits 3D tensor of shape (1, 3, 50.000) (batch, sequence_length, vocab_size)\n",
        "\n",
        "# print(outputs.logits)\n",
        "# tensor([[[ -35.2499,  -34.7863,  -38.6658,  ...,  -41.7192,  -40.9740,\n",
        "#            -35.1116],\n",
        "#          [-109.9207, -110.7950, -116.1922,  ..., -118.8029, -117.7631,\n",
        "#           -112.9502],\n",
        "#          [-110.4429, -110.4550, -113.5300,  ..., -119.2905, -115.9005,\n",
        "#           -111.2370]]], grad_fn=<UnsafeViewBackward0>)\n",
        "\n",
        "# Apply a softmax normalization to the logits to get probabilities\n",
        "# The fucntion below applies softmax independently for every [batch_index, sequence_index] pair across the vocabulary axis (the vocab_size dimension).\n",
        "probs = torch.softmax(outputs.logits, dim=-1).detach()\n",
        "\n",
        "# print(probs)\n",
        "# tensor([[[7.3724e-04, 1.1720e-03, 2.4216e-05,  ..., 1.1429e-06,\n",
        "#           2.4080e-06, 8.4655e-04],\n",
        "#          [4.7575e-05, 1.9847e-05, 8.9890e-08,  ..., 6.6052e-09,\n",
        "#           1.8684e-08, 2.2999e-06],\n",
        "#          [6.7679e-06, 6.6863e-06, 3.0885e-07,  ..., 9.7272e-10,\n",
        "#           2.8858e-08, 3.0589e-06]]])\n",
        "\n",
        "# We are only interested in the prediction of what comes after the final word\n",
        "last_position_probs = probs[:, -1, :]\n",
        "\n",
        "# Across the entire vocab, select the indices with the highest probability\n",
        "top_10_values, top_10_indices = torch.topk(last_position_probs, k=10, dim=-1)\n",
        "\n",
        "# Decode the tokens associated to those indices and print the result\n",
        "decoded_top_10 = [tokenizer.decode([idx]) for idx in top_10_indices[0]]\n",
        "print(decoded_top_10)"
      ],
      "metadata": {
        "id": "yeOVkZh_7ZMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A more interesting application is calculating the probability of the tokens like they occur in the text.\n",
        "\n",
        "We will transform the probabilities to surprisal values as is the standard practice in psycholinguistic studies."
      ],
      "metadata": {
        "id": "9gMjazzfEdSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_tokens_and_logprobs(model, tokenizer, input_texts):\n",
        "\n",
        "    # Tokenize the input\n",
        "    input_ids = tokenizer(input_texts, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Run the input through the model\n",
        "    outputs = model(input_ids)\n",
        "\n",
        "    # Apply softmax to get probabilities and take the negative logarithm to get the surprisal value\n",
        "    probs = -torch.log_softmax(outputs.logits, dim=-1).detach()\n",
        "\n",
        "    # Align token probabilities with their actual token positions\n",
        "    probs = probs[:, :-1, :]         # remove final position (we don't want prediction, only the probabilities of existing tokens)\n",
        "    input_ids = input_ids[:, 1:]     # shift input tokens by one\n",
        "\n",
        "    # Gather the probabilities corresponding to the actual generated tokens\n",
        "    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)\n",
        "\n",
        "    # Decode tokens and pair with their surprisal values\n",
        "    all_probs = []\n",
        "    for input_sentence, input_probs in zip(input_ids, gen_probs):\n",
        "        token_probs = []\n",
        "        for token, p in zip(input_sentence, input_probs):\n",
        "            if token not in tokenizer.all_special_ids:  # skip special tokens like BOS tokens\n",
        "                token_probs.append((tokenizer.decode(token), p.item()))\n",
        "        all_probs.append(token_probs)\n",
        "\n",
        "    return all_probs"
      ],
      "metadata": {
        "id": "6OIKpSgZGaqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = [\"<|endoftext|> This is an example on how to calculate each token's surprisal\"]\n",
        "probabilities = to_tokens_and_logprobs(model, tokenizer, input_text)\n",
        "\n",
        "print(probabilities)"
      ],
      "metadata": {
        "id": "bHtRIGK_EwZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More information, code and courses\n",
        "\n",
        "[Models](https://huggingface.co/models)\n",
        "\n",
        "[Hugging Face Courses](https://huggingface.co/learn/llm-course/chapter1/1)\n",
        "\n",
        "[A systematic evaluation of Dutch large language models‚Äô surprisal estimates ](https://link.springer.com/article/10.3758/s13428-025-02774-4)\n",
        "\n",
        "[Bogaertslab ](https://www.bogaertslab.com/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iyqo5ZGWKC6L"
      }
    }
  ]
}